{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this example we demonstrate how to implement a DQN agent and\n",
    "train it to trade optimally on a periodic price signal.\n",
    "Training time is short and results are unstable.\n",
    "Do not hesitate to run several times and/or tweak parameters to get better results.\n",
    "Inspired from https://github.com/keon/deep-q-learning\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tgym.envs import SpreadTrading\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 episodes,\n",
    "                 episode_length,\n",
    "                 memory_size=2000,\n",
    "                 train_interval=100,\n",
    "                 gamma=0.95,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=64,\n",
    "                 epsilon_min=0.01\n",
    "                 ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = [None] * memory_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decrement = (self.epsilon - epsilon_min)\\\n",
    "            * train_interval / (episodes * episode_length)  # linear decrease rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.brain = self._build_brain()\n",
    "        self.i = 0\n",
    "\n",
    "    def _build_brain(self):\n",
    "        \"\"\"Build the agent's brain\n",
    "        \"\"\"\n",
    "        brain = Sequential()\n",
    "        neurons_per_layer = 24\n",
    "        activation = \"relu\"\n",
    "        brain.add(Dense(neurons_per_layer,\n",
    "                        input_dim=self.state_size,\n",
    "                        activation=activation))\n",
    "        brain.add(Dense(neurons_per_layer, activation=activation))\n",
    "        brain.add(Dense(self.action_size, activation='linear'))\n",
    "        brain.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return brain\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Acting Policy of the DQNAgent\n",
    "        \"\"\"\n",
    "        action = np.zeros(self.action_size)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action[random.randrange(self.action_size)] = 1\n",
    "        else:\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            act_values = self.brain.predict(state)\n",
    "            action[np.argmax(act_values[0])] = 1\n",
    "        return action\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, done, warming_up=False):\n",
    "        \"\"\"Memory Management and training of the agent\n",
    "        \"\"\"\n",
    "        self.i = (self.i + 1) % self.memory_size\n",
    "        self.memory[self.i] = (state, action, reward, next_state, done)\n",
    "        if (not warming_up) and (self.i % self.train_interval) == 0:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decrement\n",
    "            state, action, reward, next_state, done = self._get_batches()\n",
    "            reward += (self.gamma\n",
    "                       * np.logical_not(done)\n",
    "                       * np.amax(self.brain.predict(next_state),\n",
    "                                 axis=1))\n",
    "            q_target = self.brain.predict(state)\n",
    "            #print \"state: \", state[0]\n",
    "            #print \"action[0]: \", action[0]\n",
    "            #print \"action[1]: \", action[1]\n",
    "            #print \"q_target: \", q_target[action[0], action[1]]\n",
    "            #print \"reward: \", reward\n",
    "            \n",
    "            q_target[action[0], action[1]] = reward\n",
    "            return self.brain.fit(state, q_target,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  epochs=1,\n",
    "                                  verbose=False)\n",
    "\n",
    "    def _get_batches(self):\n",
    "        \"\"\"Selecting a batch of memory\n",
    "           Split it into categorical subbatches\n",
    "           Process action_batch into a position vector\n",
    "        \"\"\"\n",
    "        batch = np.array(random.sample(self.memory, self.batch_size))\n",
    "        state_batch = np.concatenate(batch[:, 0])\\\n",
    "            .reshape(self.batch_size, self.state_size)\n",
    "        action_batch = np.concatenate(batch[:, 1])\\\n",
    "            .reshape(self.batch_size, self.action_size)\n",
    "        reward_batch = batch[:, 2]\n",
    "        next_state_batch = np.concatenate(batch[:, 3])\\\n",
    "            .reshape(self.batch_size, self.state_size)\n",
    "        done_batch = batch[:, 4]\n",
    "        # action processing\n",
    "        action_batch = np.where(action_batch == 1)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/Users/matthewdixon/Downloads/Trading-Gym/')\n",
    "from tgym.envs import SpreadTrading\n",
    "#from tgym.gens.deterministic import WavySignal\n",
    "#from tgym.gens.random import AR1\n",
    "from tgym.gens.csvstream import CSVStreamer\n",
    "# Instantiating the environmnent\n",
    "generator = CSVStreamer(filename='../../data/AMZN-L1.csv')\n",
    "#generator = AR1(a=0.1, ba_spread=0.1)   #WavySignal(period_1=25, period_2=50, epsilon=-0.5)\n",
    "episodes = 100\n",
    "episode_length = 400\n",
    "trading_fee = .0\n",
    "time_fee = 0\n",
    "history_length = 2\n",
    "environment = SpreadTrading(spread_coefficients=[1],\n",
    "                            data_generator=generator,\n",
    "                                trading_fee=trading_fee,\n",
    "                                time_fee=time_fee,\n",
    "                                history_length=history_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_trainable': False,\n",
       " 'gen_kwargs': {'a': 0.1, 'ba_spread': 0.1},\n",
       " 'generator': <generator object _generator at 0x115692e10>,\n",
       " 'n_products': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = AR1(a=0.1, ba_spread=0.1)   #WavySignal(period_1=25, period_2=50, epsilon=-0.5)\n",
    "generator.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_action': array([1, 0, 0]),\n",
       " '_closed_plot': False,\n",
       " '_data_generator': <tgym.gens.random.AR1 at 0x11568b490>,\n",
       " '_depths_history': [(1000, 1000), (988, 1006)],\n",
       " '_entry_price': 0,\n",
       " '_episode_length': 1000,\n",
       " '_exit_price': 0,\n",
       " '_first_render': True,\n",
       " '_history_length': 2,\n",
       " '_iteration': 0,\n",
       " '_position': array([1, 0, 0]),\n",
       " '_prices_history': [(100, 100.1), (8.985108107208035, 9.085108107208034)],\n",
       " '_spread_coefficients': [1],\n",
       " '_time_fee': 0,\n",
       " '_total_pnl': 0,\n",
       " '_total_reward': 0,\n",
       " '_trading_fee': 0.2,\n",
       " 'n_actions': 3,\n",
       " 'state_shape': (12,)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+02, 1.00100000e+02, 9.73035592e+00, 9.83035592e+00,\n",
       "       1.00000000e+03, 1.00000000e+03, 1.00000000e+03, 1.00200000e+03,\n",
       "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = environment.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:0| rew:-10.47| eps:0.99| loss:1378.1431\n",
      "Ep:1| rew:-0.98| eps:0.98| loss:91.3164\n",
      "Ep:2| rew:-11.04| eps:0.97| loss:49.2645\n",
      "Ep:3| rew:-2.39| eps:0.96| loss:57.4468\n",
      "Ep:4| rew:1.88| eps:0.95| loss:146.9944\n",
      "Ep:5| rew:-7.58| eps:0.94| loss:140.1393\n",
      "Ep:6| rew:-14.06| eps:0.93| loss:100.5125\n",
      "Ep:7| rew:-7.6| eps:0.92| loss:21.5093\n",
      "Ep:8| rew:2.42| eps:0.91| loss:14.273\n",
      "Ep:9| rew:-7.62| eps:0.9| loss:13.6398\n",
      "Ep:10| rew:-12.16| eps:0.89| loss:3.9049\n",
      "Ep:11| rew:-18.32| eps:0.88| loss:3.8552\n",
      "Ep:12| rew:-17.57| eps:0.87| loss:2.1659\n",
      "Ep:13| rew:-24.8| eps:0.86| loss:2.699\n",
      "Ep:14| rew:-9.16| eps:0.85| loss:1.6074\n",
      "Ep:15| rew:-1.8| eps:0.84| loss:1.6647\n",
      "Ep:16| rew:-7.9| eps:0.83| loss:1.6519\n",
      "Ep:17| rew:15.37| eps:0.82| loss:1.6424\n",
      "Ep:18| rew:1.57| eps:0.81| loss:1.4861\n",
      "Ep:19| rew:-33.38| eps:0.8| loss:1.424\n",
      "Ep:20| rew:-34.96| eps:0.79| loss:1.2017\n",
      "Ep:21| rew:-11.38| eps:0.78| loss:1.7213\n",
      "Ep:22| rew:5.02| eps:0.77| loss:1.2855\n",
      "Ep:23| rew:-21.87| eps:0.76| loss:3.0298\n",
      "Ep:24| rew:-4.47| eps:0.75| loss:1.8558\n",
      "Ep:25| rew:-13.37| eps:0.74| loss:1.4332\n",
      "Ep:26| rew:-1.56| eps:0.73| loss:1.1833\n",
      "Ep:27| rew:-9.63| eps:0.72| loss:1.9927\n",
      "Ep:28| rew:-24.11| eps:0.71| loss:2.2796\n",
      "Ep:29| rew:-2.86| eps:0.7| loss:1.5423\n",
      "Ep:30| rew:-20.46| eps:0.69| loss:1.6496\n",
      "Ep:31| rew:2.57| eps:0.68| loss:1.565\n",
      "Ep:32| rew:-5.77| eps:0.67| loss:1.5499\n",
      "Ep:33| rew:7.89| eps:0.66| loss:1.649\n",
      "Ep:34| rew:11.03| eps:0.65| loss:1.2141\n",
      "Ep:35| rew:-20.96| eps:0.64| loss:1.2359\n",
      "Ep:36| rew:12.65| eps:0.63| loss:1.7659\n",
      "Ep:37| rew:10.25| eps:0.62| loss:1.3445\n",
      "Ep:38| rew:3.28| eps:0.61| loss:2.0886\n",
      "Ep:39| rew:1.62| eps:0.6| loss:0.6848\n",
      "Ep:40| rew:-17.71| eps:0.59| loss:0.7834\n",
      "Ep:41| rew:-9.58| eps:0.58| loss:0.8726\n",
      "Ep:42| rew:-0.55| eps:0.57| loss:1.4016\n",
      "Ep:43| rew:-22.96| eps:0.56| loss:1.3669\n",
      "Ep:44| rew:5.87| eps:0.55| loss:1.1745\n",
      "Ep:45| rew:-18.22| eps:0.54| loss:1.0622\n",
      "Ep:46| rew:-11.28| eps:0.53| loss:1.0961\n",
      "Ep:47| rew:-24.58| eps:0.52| loss:1.2248\n",
      "Ep:48| rew:-11.19| eps:0.51| loss:1.4693\n",
      "Ep:49| rew:-6.63| eps:0.51| loss:1.3083\n",
      "Ep:50| rew:-30.87| eps:0.5| loss:0.982\n",
      "Ep:51| rew:-29.51| eps:0.49| loss:2.0579\n",
      "Ep:52| rew:-31.96| eps:0.48| loss:0.9619\n",
      "Ep:53| rew:-5.95| eps:0.47| loss:1.537\n",
      "Ep:54| rew:-4.61| eps:0.46| loss:1.2198\n",
      "Ep:55| rew:-0.83| eps:0.45| loss:1.3047\n",
      "Ep:56| rew:8.55| eps:0.44| loss:1.069\n",
      "Ep:57| rew:-8.98| eps:0.43| loss:1.3821\n",
      "Ep:58| rew:-25.89| eps:0.42| loss:0.9037\n",
      "Ep:59| rew:-31.77| eps:0.41| loss:0.9122\n",
      "Ep:60| rew:-7.65| eps:0.4| loss:0.7292\n",
      "Ep:61| rew:-31.87| eps:0.39| loss:0.8869\n",
      "Ep:62| rew:-10.71| eps:0.38| loss:0.8823\n",
      "Ep:63| rew:-31.51| eps:0.37| loss:1.1607\n",
      "Ep:64| rew:9.82| eps:0.36| loss:0.7275\n",
      "Ep:65| rew:5.82| eps:0.35| loss:1.5574\n",
      "Ep:66| rew:-21.57| eps:0.34| loss:0.9573\n",
      "Ep:67| rew:0.15| eps:0.33| loss:2.1999\n",
      "Ep:68| rew:-21.01| eps:0.32| loss:0.6883\n",
      "Ep:69| rew:13.49| eps:0.31| loss:0.8595\n",
      "Ep:70| rew:-8.3| eps:0.3| loss:0.6422\n",
      "Ep:71| rew:-23.75| eps:0.29| loss:1.0867\n",
      "Ep:72| rew:-14.85| eps:0.28| loss:0.4599\n",
      "Ep:73| rew:-13.04| eps:0.27| loss:0.7345\n",
      "Ep:74| rew:-20.42| eps:0.26| loss:1.2316\n",
      "Ep:75| rew:5.54| eps:0.25| loss:0.5973\n",
      "Ep:76| rew:-3.83| eps:0.24| loss:0.701\n",
      "Ep:77| rew:9.37| eps:0.23| loss:0.6484\n",
      "Ep:78| rew:1.34| eps:0.22| loss:0.6329\n",
      "Ep:79| rew:10.23| eps:0.21| loss:0.588\n",
      "Ep:80| rew:-29.51| eps:0.2| loss:0.8223\n",
      "Ep:81| rew:-0.85| eps:0.19| loss:0.5918\n",
      "Ep:82| rew:-5.53| eps:0.18| loss:0.8432\n",
      "Ep:83| rew:-16.41| eps:0.17| loss:0.4102\n",
      "Ep:84| rew:9.3| eps:0.16| loss:0.5606\n",
      "Ep:85| rew:-3.6| eps:0.15| loss:0.7842\n",
      "Ep:86| rew:2.99| eps:0.14| loss:0.5484\n",
      "Ep:87| rew:-10.39| eps:0.13| loss:0.3978\n",
      "Ep:88| rew:9.09| eps:0.12| loss:0.5035\n",
      "Ep:89| rew:-24.76| eps:0.11| loss:1.2057\n",
      "Ep:90| rew:8.54| eps:0.1| loss:0.3551\n",
      "Ep:91| rew:-8.31| eps:0.09| loss:0.4895\n",
      "Ep:92| rew:5.61| eps:0.08| loss:0.3787\n",
      "Ep:93| rew:5.42| eps:0.07| loss:0.6729\n",
      "Ep:94| rew:-19.98| eps:0.06| loss:0.4264\n",
      "Ep:95| rew:-16.01| eps:0.05| loss:0.2912\n",
      "Ep:96| rew:-18.89| eps:0.04| loss:0.8786\n",
      "Ep:97| rew:-8.53| eps:0.03| loss:0.3611\n",
      "Ep:98| rew:14.65| eps:0.02| loss:0.4137\n",
      "Ep:99| rew:2.07| eps:0.01| loss:0.487\n"
     ]
    }
   ],
   "source": [
    "state = environment.reset()\n",
    "# Instantiating the agent\n",
    "memory_size = 3000\n",
    "state_size = len(state)\n",
    "gamma = 0.96\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "action_size = len(SpreadTrading._actions)\n",
    "train_interval = 10\n",
    "learning_rate = 0.001\n",
    "agent = DQNAgent(state_size=state_size,\n",
    "                     action_size=action_size,\n",
    "                     memory_size=memory_size,\n",
    "                     episodes=episodes,\n",
    "                     episode_length=episode_length,\n",
    "                     train_interval=train_interval,\n",
    "                     gamma=gamma,\n",
    "                     learning_rate=learning_rate,\n",
    "                     batch_size=batch_size,\n",
    "                     epsilon_min=epsilon_min)\n",
    "# Warming up the agent\n",
    "for _ in range(memory_size):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "        agent.observe(state, action, reward, next_state, done, warming_up=True)\n",
    "# Training the agent\n",
    "for ep in range(episodes):\n",
    "    state = environment.reset()\n",
    "    rew = 0\n",
    "    for _ in range(episode_length):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "        loss = agent.observe(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        rew += reward\n",
    "    print(\"Ep:\" + str(ep)\n",
    "           + \"| rew:\" + str(round(rew, 2))\n",
    "           + \"| eps:\" + str(round(agent.epsilon, 2))\n",
    "           + \"| loss:\" + str(round(loss.history[\"loss\"][0], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running the agent\n",
    "done = False\n",
    "state = environment.reset()\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    state, _, done, info = environment.step(action)\n",
    "    if 'status' in info and info['status'] == 'Closed plot':\n",
    "        done = True\n",
    "    else:\n",
    "        environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
